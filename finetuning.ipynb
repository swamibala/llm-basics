{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f891c6",
   "metadata": {},
   "source": [
    "# Finetuning a LLM in Macbook Pro\n",
    "### System Configuration\n",
    "- Macbook Pro M4 Pro\n",
    "- 48GB RAM\n",
    "- GPU Cores: 20\n",
    "- Metal Support: Metal 4\n",
    "\n",
    "### MPS (Metal Performance Shaders): \n",
    "- Apple-provided GPU backend that PyTorch can use on Apple Silicon. Using MPS speeds training/inference compared to CPU.\n",
    "- PyTorch tensors and model weights can be moved to device=\"mps\"\n",
    "- Forward + backward passes happen on the GPU → much faster than CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d32cb",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "```\n",
    "uv init .venv --python=3.12\n",
    "source .venv/bin/activate\n",
    "uv add ipykernel jupyter\n",
    "uv add torch torchvision torchaudio\n",
    "uv add transformers accelerate datasets huggingface_hub safetensors\n",
    "\n",
    "# Register your venv as a fresh kernel\n",
    "python -m ipykernel install --user \\\n",
    "  --name=gemma270m \\\n",
    "  --display-name \"Gemma 270M (venv)\"\n",
    "```\n",
    "\n",
    "Restart IDE & select kernel: Gemma 270M (venv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6481bfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS built: True\n"
     ]
    }
   ],
   "source": [
    "# Check whether MPS is available to train on Apple GPU (Metal):\n",
    "import torch\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec2e8a",
   "metadata": {},
   "source": [
    "### Configure Hugging Face authentication\n",
    "In terminal run\n",
    "```\n",
    "huggingface-cli login\n",
    "```\n",
    "paste your token from https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b4c03",
   "metadata": {},
   "source": [
    "### Load the model and the tokenizer\n",
    "- Tokenizer:\n",
    "  - Maps words/subwords → integers\n",
    "  - Handles special tokens like <pad>, <eos>\n",
    "  - Ensures input sequences are exactly what the model expects\n",
    "\n",
    "- Model:\n",
    "  - PyTorch object containing all 270M weights\n",
    "  - Decoder-only transformer: takes token IDs → predicts next token logits\n",
    "  - from_pretrained downloads weights, config, and merges all necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d86f7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"google/functiongemma-270m-it\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412025a2",
   "metadata": {},
   "source": [
    "### Move model to GPU (MPS)\n",
    "- We want PyTorch to use MPS (Metal Performance Shaders) for GPU acceleration. Mac M4 Pro has 20 GPU cores.\n",
    "- `model.to(device)` moves all weights to GPU memory\n",
    "- Forward pass (computing predictions) + backward pass (gradients) happen on GPU\n",
    "- Using CPU is much slower because PyTorch cannot parallelize large matrix multiplications as efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b80c6e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e683ab",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa3ff22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Tool Definition for a Travel Planner ---\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web using DuckDuckGo.\n",
    "\n",
    "    Args:\n",
    "        query: query string\n",
    "    Returns:\n",
    "        Top web result as string\n",
    "    \"\"\"\n",
    "    return f\"Search result for: {query}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b47d0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the travel planner dataset\n",
    "# Each example represents user intent and the function call the model should make.\n",
    "\n",
    "simple_tool_calling = [\n",
    "    {\n",
    "        \"user_content\": \"Best places to visit in Switzerland for 5 days\",\n",
    "        \"tool_name\": \"search_web\",\n",
    "        \"tool_arguments\": '{\"query\": \"best places to visit in Switzerland for 5 days\"}'\n",
    "    },\n",
    "    {\n",
    "        \"user_content\": \"Best time to visit Japan for cherry blossoms\",\n",
    "        \"tool_name\": \"search_web\",\n",
    "        \"tool_arguments\": '{\"query\": \"best time to visit Japan for cherry blossoms\"}'\n",
    "    },\n",
    "    {\n",
    "        \"user_content\": \"Top day trips from Zurich suitable for a 5-day Switzerland vacation\",\n",
    "        \"tool_name\": \"search_web\",\n",
    "        \"tool_arguments\": '{\"query\": \"top day trips from Zurich for a 5 day Switzerland vacation\"}'\n",
    "    },\n",
    "    {\n",
    "        \"user_content\": \"Family-friendly 5-day Switzerland trip with easy trains and kids activities\",\n",
    "        \"tool_name\": \"search_web\",\n",
    "        \"tool_arguments\": '{\"query\": \"family friendly 5 day Switzerland trip trains kids activities\"}'\n",
    "    },\n",
    "    {\n",
    "        \"user_content\": \"Top-rated car rental services in Los Angeles\",\n",
    "        \"tool_name\": \"search_web\",\n",
    "        \"tool_arguments\": '{\"query\": \"top rated car rental services in Los Angeles\"}'\n",
    "    },\n",
    "    {\n",
    "        \"user_content\": \"Find family-friendly hotels in Paris for 2 nights\",\n",
    "        \"tool_name\": \"search_web\",\n",
    "        \"tool_arguments\": '{\"query\": \"family friendly hotels in Paris for 2 nights\"}'\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00b2eb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e8b710af1749df9b5154823e0f63ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare Dataset for training\n",
    "from transformers.utils import get_json_schema\n",
    "\n",
    "# Convert function to schema for model awareness - lets the model understand the function signature\n",
    "TOOLS = [get_json_schema(search_web)]\n",
    "\n",
    "DEFAULT_SYSTEM_MSG = (\n",
    "    \"You are a travel assistant that MUST use the available tools.\\n\"\n",
    "    \"You are NOT allowed to refuse travel-related questions.\\n\"\n",
    "    \"For ANY travel planning, destination, itinerary, transport, or activity question, \"\n",
    "    \"you MUST call the `search_web` function.\\n\"\n",
    "    \"Do NOT ask follow-up questions.\\n\"\n",
    "    \"Do NOT provide explanations.\\n\"\n",
    "    \"ONLY return a function call.\"\n",
    ")\n",
    "\n",
    "# Map each raw sample to conversational format. converts each sample into messages + tool call format.\n",
    "def create_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"developer\", \"content\": DEFAULT_SYSTEM_MSG},\n",
    "            {\"role\": \"user\", \"content\": sample[\"user_content\"]},\n",
    "            {\"role\": \"assistant\", \"tool_calls\": [\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": sample[\"tool_name\"],\n",
    "                        \"arguments\": json.loads(sample[\"tool_arguments\"])\n",
    "                    }\n",
    "                }\n",
    "            ]}\n",
    "        ],\n",
    "        \"tools\": TOOLS\n",
    "    }\n",
    "\n",
    "# Convert list to Hugging Face dataset\n",
    "dataset = Dataset.from_list(simple_tool_calling)\n",
    "\n",
    "# Map to conversation format\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n",
    "\n",
    "# Split into train/test (50/50)\n",
    "dataset = dataset.train_test_split(test_size=0.5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebb484",
   "metadata": {},
   "source": [
    "### Test Before Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97f0d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Prompt: Family-friendly 5-day Switzerland trip with easy trains and kids activities\n",
      "   Output: <start_function_call>call:search_web{query:<escape>family-friendly 5-day Switzerland trip with easy trains and kids activities<escape>}<end_function_call><start_function_response>\n",
      "   -> ✅ correct!\n",
      "2. Prompt: Top day trips from Zurich suitable for a 5-day Switzerland vacation\n",
      "   Output: <start_function_call>call:search_web{query:<escape>Top day trips from Zurich suitable for a 5-day Switzerland vacation<escape>}<end_function_call><start_function_response>\n",
      "   -> ✅ correct!\n",
      "3. Prompt: Find family-friendly hotels in Paris for 2 nights\n",
      "   Output: <start_function_call>call:search_web{query:<escape>family-friendly hotels in Paris<escape>}<end_function_call><start_function_response>\n",
      "   -> ✅ correct!\n",
      "\n",
      "Overall Success: 3 / 3\n"
     ]
    }
   ],
   "source": [
    "def check_success_rate():\n",
    "    success_count = 0\n",
    "    for idx, item in enumerate(dataset['test']):\n",
    "        # Take system + user messages only\n",
    "        messages = [\n",
    "            item[\"messages\"][0],  # system message\n",
    "            item[\"messages\"][1],  # user message\n",
    "        ]\n",
    "\n",
    "        # Prepare inputs using chat template\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools=TOOLS,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Generate model output\n",
    "        out = model.generate(\n",
    "            **inputs.to(model.device),\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=128\n",
    "        )\n",
    "\n",
    "        # Decode generated tokens\n",
    "        output = tokenizer.decode(\n",
    "            out[0][len(inputs[\"input_ids\"][0]):],\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        print(f\"{idx+1}. Prompt: {item['messages'][1]['content']}\")\n",
    "        print(f\"   Output: {output}\")\n",
    "\n",
    "        # Check if the model called the correct tool\n",
    "        expected_tool = item['messages'][2]['tool_calls'][0]['function']['name']\n",
    "\n",
    "        if expected_tool in output:\n",
    "            print(\"   -> ✅ correct!\")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"   -> ❌ wrong (expected '{expected_tool}' missing)\")\n",
    "\n",
    "    print(f\"\\nOverall Success: {success_count} / {len(dataset['test'])}\")\n",
    "\n",
    "check_success_rate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d76e0",
   "metadata": {},
   "source": [
    "### Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA)\n",
    "- PEFT (Parameter-Efficient Fine-Tuning) is a method to adapt large pretrained models (like LLMs) to new tasks without retraining all model parameters.\n",
    "- Instead of updating billions of weights, most of the base model is frozen, which greatly reduces training time, memory usage, and hardware requirements.\n",
    "- LoRA (Low-Rank Adaptation) is a popular PEFT technique that adds small, trainable low-rank matrices to selected layers of the model.\n",
    "- During training, only these added LoRA parameters are updated, while the original model weights remain unchanged.\n",
    "- This allows the model to learn task-specific behavior efficiently, often achieving performance close to full fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1935cc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swamibala/Code/llm-basics/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swamibala/Code/llm-basics/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in ./functiongemma_merged - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter saved to ./functiongemma_lora_adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swamibala/Code/llm-basics/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in ./functiongemma_merged - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, set_peft_model_state_dict\n",
    "from datasets import Dataset\n",
    "\n",
    "# Ensure tokenizer has eos_token & pad_token. Causal LM training requires consistent special tokens and padding.\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({\"eos_token\": \"\"})\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ===== PEFT/LoRA config =====\n",
    "# r, alpha, and target_modules control the adapter capacity and where adapters are inserted\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # low-rank dimension (tweakable)\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical transformer proj names; OK for Gemma-style\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Wrap model with LoRA (freezes base params by default)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ===== Prepare the dataset: create input_ids and labels =====\n",
    "# We will build prompt = tokenizer.apply_chat_template(system+user, tools=TOOLS, add_generation_prompt=True)\n",
    "# and target = serialized assistant tool-call string (from dataset[\"messages\"][2]).\n",
    "# Helper to serialize expected assistant tool call in the same format the model uses:\n",
    "def serialize_tool_call(tool_call):\n",
    "    # tool_call: dict with \"function\": {\"name\": ..., \"arguments\": {...}}\n",
    "    name = tool_call[\"function\"][\"name\"]\n",
    "    args = tool_call[\"function\"][\"arguments\"]\n",
    "    # For simplicity we assume only a \"query\" arg exists (as in your dataset)\n",
    "    # Escape braces/curly to mimic the training format used earlier\n",
    "    q = args.get(\"query\", \"\")\n",
    "    # Build the same textual format observed earlier:\n",
    "    return f\"<start_function_call>call:{name}{{query:<escape>{q}<escape>}}<end_function_call><start_function_response>\"\n",
    "\n",
    "# Map HF dataset to tokenized training samples\n",
    "def prepare_example(example):\n",
    "    # example[\"messages\"] exists (developer, user, assistant)\n",
    "    system_msg = example[\"messages\"][0]\n",
    "    user_msg = example[\"messages\"][1]\n",
    "    assistant_tool_call = example[\"messages\"][2][\"tool_calls\"][0]\n",
    "\n",
    "    # Build prompt using tokenizer helper (you used apply_chat_template earlier)\n",
    "    prompt_inputs = tokenizer.apply_chat_template(\n",
    "        [system_msg, user_msg],\n",
    "        tools=TOOLS,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    prompt_ids = prompt_inputs[\"input_ids\"][0]  # tensor\n",
    "    # Build target string and tokenize it\n",
    "    target_text = serialize_tool_call(assistant_tool_call)\n",
    "    target_ids = tokenizer(target_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    # Concatenate prompt + target for labels. We will mask prompt tokens with -100 so loss only computed on target tokens.\n",
    "    input_ids = torch.cat([prompt_ids, target_ids], dim=0)\n",
    "    labels = torch.cat([torch.full_like(prompt_ids, -100), target_ids], dim=0)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# Apply to dataset (this converts to torch tensors). Dataset currently is a dict with train/test splits.\n",
    "def dataset_to_train_format(hf_dataset):\n",
    "    # expects hf_dataset is HF dataset split (list-like)\n",
    "    mapped = {\"input_ids\": [], \"labels\": []}\n",
    "    for ex in hf_dataset:\n",
    "        prepared = prepare_example(ex)\n",
    "        mapped[\"input_ids\"].append(prepared[\"input_ids\"])\n",
    "        mapped[\"labels\"].append(prepared[\"labels\"])\n",
    "    # Create a small huggingface Dataset with tensors\n",
    "    # We will store lists of tensors and then rely on a data collator to pad them\n",
    "    return Dataset.from_dict({\"input_ids\": mapped[\"input_ids\"], \"labels\": mapped[\"labels\"]})\n",
    "\n",
    "train_ds = dataset_to_train_format(dataset[\"train\"])\n",
    "eval_ds  = dataset_to_train_format(dataset[\"test\"])\n",
    "\n",
    "# ===== Data collator: pads input_ids and labels to same length =====\n",
    "class DataCollatorForCausalWithPadding:\n",
    "    def __init__(self, tokenizer, pad_token_id=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id if pad_token_id is not None else tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # features: list of {\"input_ids\": tensor, \"labels\": tensor}\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
    "        labels = [torch.tensor(f[\"labels\"]) for f in features]\n",
    "        # pad to longest length\n",
    "        max_len = max(t.size(0) for t in input_ids)\n",
    "        padded_inputs = []\n",
    "        padded_labels = []\n",
    "        for inp, lab in zip(input_ids, labels):\n",
    "            pad_len = max_len - inp.size(0)\n",
    "            padded_inputs.append(torch.cat([inp, torch.full((pad_len,), self.pad_token_id, dtype=torch.long)]))\n",
    "            padded_labels.append(torch.cat([lab, torch.full((pad_len,), -100, dtype=torch.long)]))\n",
    "        batch = {\"input_ids\": torch.stack(padded_inputs), \"labels\": torch.stack(padded_labels)}\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorForCausalWithPadding(tokenizer)\n",
    "\n",
    "# ===== Training arguments =====\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./functiongemma_lora_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    fp16=False,  # fp16 on MPS is often problematic; keep float32\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# ===== Trainer =====\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ===== Run training =====\n",
    "trainer.train()\n",
    "\n",
    "# ===== Save LoRA adapter weights (and optionally full merged model) =====\n",
    "model.save_pretrained(\"./functiongemma_lora_adapter\")\n",
    "tokenizer.save_pretrained(\"./functiongemma_lora_adapter\")\n",
    "print(\"LoRA adapter saved to ./functiongemma_lora_adapter\")\n",
    "# This is just the adapter weights, not the full base model.\n",
    "# We can merge later if needed, but using LoRA keeps it lightweight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b609e54",
   "metadata": {},
   "source": [
    "### Merge LoRA Adapter and Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e9841dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./functiongemma_merged/tokenizer_config.json',\n",
       " './functiongemma_merged/special_tokens_map.json',\n",
       " './functiongemma_merged/chat_template.jinja',\n",
       " './functiongemma_merged/tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load base model fresh\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_name = \"google/functiongemma-270m-it\"\n",
    "lora_path = \"./functiongemma_lora_adapter\"\n",
    "merged_output_path = \"./functiongemma_merged\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "#Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_path\n",
    ")\n",
    "\n",
    "#Merge and unload LoRA\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "#Save merged model to get standalone fine-tuned model\n",
    "merged_model.save_pretrained(merged_output_path)\n",
    "tokenizer.save_pretrained(merged_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c2806",
   "metadata": {},
   "source": [
    "### Test After Finetuning by loading the merged model (no PEFT anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "720afafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './functiongemma_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Prompt: Family-friendly 5-day Switzerland trip with easy trains and kids activities\n",
      "   Output: <start_function_call>call:search_web{query:<escape>family-friendly 5-day Switzerland trip with easy trains and kids activities<escape>}<end_function_call><start_function_response>\n",
      "   -> ✅ correct!\n",
      "2. Prompt: Top day trips from Zurich suitable for a 5-day Switzerland vacation\n",
      "   Output: <start_function_call>call:search_web{query:<escape>top day trips from Zurich suitable for a 5-day Switzerland vacation<escape>}<end_function_call><start_function_response>\n",
      "   -> ✅ correct!\n",
      "3. Prompt: Find family-friendly hotels in Paris for 2 nights\n",
      "   Output: <start_function_call>call:search_web{query:<escape>family-friendly hotels in Paris<escape>}<end_function_call><start_function_response>\n",
      "   -> ✅ correct!\n",
      "\n",
      "Overall Success: 3 / 3\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./functiongemma_merged\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./functiongemma_merged\",\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "check_success_rate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-basics)",
   "language": "python",
   "name": "llm-basics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
