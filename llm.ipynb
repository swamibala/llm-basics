{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212d6aad",
   "metadata": {},
   "source": [
    "# Building Blocks of Language Models\n",
    "In this notebook, we will explore:\n",
    "- <b>Tokenization</b>: Breaking text into tokens\n",
    "- <b>Embeddings</b>: Converting tokens to numerical vectors\n",
    "- <b>Positional Encoding</b>: Adding position information\n",
    "- <b>Self-Attention</b>: Using all of the above to understand relationships\n",
    "- <b>Prediction</b>: Generating the next token using the model's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8feb0",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- The model can‚Äôt read words directly ‚Äî it only understands numbers. So we need to convert text ‚Üí tokens ‚Üí numbers.\n",
    "- tokenize() splits into sub-word pieces.\n",
    "- encode() converts them into numeric IDs.\n",
    "- decode() reverses the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f137009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a small open model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Small Language Models are powerful and efficient!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)\n",
    "print(\"Decoded back:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b60004",
   "metadata": {},
   "source": [
    "### A Simple Tokenizer Implementation (Toy Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    # Lowercase + split on spaces & punctuation\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "    return tokens\n",
    "\n",
    "sample_text = \"Small Language Models are powerful and efficient!\"\n",
    "tokens = basic_tokenizer(sample_text)\n",
    "print(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663791a",
   "metadata": {},
   "source": [
    "### Real GPT-2 BPE tokenizer\n",
    "1. GPT-2 uses subword tokenization (Byte Pair Encoding, BPE) to handle rare words efficiently.\n",
    "2. Tokens are not always whole words; BPE splits or merges based on frequency.\n",
    "3. The tokenizer‚Äôs vocabulary is fixed, built before model training. \n",
    "   Tokenizer training is often done on a subset of the full corpus (e.g., 1‚Äì10% of data), if the corpus is extremely large, to save time.\n",
    "   The subset must be representative of all text the model will encounter.\n",
    "   The vocabulary generated is then fixed and used throughout model training.\n",
    "\n",
    "### Analogy\n",
    "- Think of the vocabulary as a dictionary for the model.\n",
    "- You want the dictionary to cover all common words in the language you‚Äôre going to teach the model.\n",
    "- Making a dictionary from Shakespeare when you‚Äôre training on Wikipedia would work poorly ‚Äî it won‚Äôt reflect the frequency of words in Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428238c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer (uses real BPE)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Small Language Models are powerful and efficient!\"\n",
    "tokens_hf = hf_tokenizer.tokenize(text)\n",
    "ids_hf = hf_tokenizer.encode(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"\\nHugging Face BPE Tokens:\", tokens_hf)\n",
    "print(\"Token IDs:\", ids_hf)\n",
    "print(\"Decoded back:\", hf_tokenizer.decode(ids_hf))\n",
    "# Decoding some random token ID - Prints a word referred in the tokenizer vocabulary\n",
    "print(\"Random Token:\", hf_tokenizer.decode(18710))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d9d0f",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "### Definition of embedding:\n",
    "* Numerical vector representing an object (word, token, or sentence) in high-dimensional space.\n",
    "* Similar objects ‚Üí vectors close together; dissimilar ‚Üí far apart.\n",
    "\n",
    "### Purpose in Transformers / LLMs:\n",
    "* Converts symbolic text into numbers so the model can process it.\n",
    "* Captures semantic meaning and, in last_hidden_state, contextual meaning.\n",
    "\n",
    "#### Example:\n",
    "- Text: \"Small Language Models\"\n",
    "- Tokens: [\"Small\", \"ƒ†Language\", \"ƒ†Models\"]\n",
    "- Each token ‚Üí 768-dimensional vector (GPT-2 small).\n",
    "\n",
    "### Why embeddings are important:\n",
    "* Provide contextualization: combined with attention, the model ‚Äúunderstands‚Äù meaning in context.\n",
    "* Allow semantic similarity: similar words (e.g., ‚Äúking‚Äù & ‚Äúqueen‚Äù) have similar vectors.\n",
    "* Serve as the starting point for downstream tasks: next-token prediction, classification, etc.\n",
    "\n",
    "### Analogy:\n",
    "* Think of a map of cities\n",
    "* Each city = token\n",
    "* Coordinates = embedding vector\n",
    "* Nearby cities = similar meaning\n",
    "* Moving through the map = attention + feed-forward layers updating embeddings\n",
    "\n",
    "### last_hidden_state:\n",
    "* Tensor of shape [batch_size, seq_len, hidden_size]\n",
    "* Each token has a contextualized embedding vector after passing through Transformer layers.\n",
    "* Before passing through layers ‚Üí embeddings are basic numeric representations of tokens.\n",
    "* After layers ‚Üí embeddings are contextualized, rich representations used for predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28677051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Embeddings from GPT-2\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Small Language Models are powerful and efficient!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # shape: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n",
    "print(\"Number of tokens:\", embeddings.shape[1])\n",
    "print(\"Hidden size:\", embeddings.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Embeddings with PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "emb_np = embeddings[0].numpy()  # convert from tensor to numpy\n",
    "\n",
    "# Reduce 768-d to 2D\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(emb_np)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(emb_2d[:,0], emb_2d[:,1])\n",
    "for i, token in enumerate(tokens):\n",
    "    plt.annotate(token, (emb_2d[i,0], emb_2d[i,1]))\n",
    "plt.title(\"2D PCA of GPT-2 Token Embeddings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2fb20",
   "metadata": {},
   "source": [
    "## Positional Encoding (Sinusoidal + Learned)\n",
    "Transformers don‚Äôt process words sequentially like RNNs.\n",
    "So they need to know where each token is in the sentence.\n",
    "\n",
    "They add a positional vector (e.g., sine/cosine pattern or learned embedding)\n",
    "so ‚ÄúThe cat‚Äù ‚â† ‚Äúcat The‚Äù.\n",
    "\n",
    "Result = (embedding + position) for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af708ca",
   "metadata": {},
   "source": [
    "### 1. Sinusoidal Positional Encoding\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f95530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encodings of shape (seq_len, d_model)\n",
    "    As described in 'Attention is All You Need'\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    # frequencies: 10000^(2i/d_model)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "    # apply sin to even indices, cos to odd\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db6df2",
   "metadata": {},
   "source": [
    "#### Visualize Sinusoidal Encoding Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq_len = 1024\n",
    "d_model = 768\n",
    "\n",
    "pe = sinusoidal_positional_encoding(seq_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe, aspect='auto', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Sinusoidal Positional Encoding (1024 positions √ó 768 dims)\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0748e2",
   "metadata": {},
   "source": [
    "#### Extract GPT-2 Token Embeddings for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c276c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"The dog chased the ball.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    token_emb = outputs.last_hidden_state[0]  # shape: (seq_len, d_model)\n",
    "\n",
    "print(\"Token embedding shape:\", token_emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b209f",
   "metadata": {},
   "source": [
    "#### Add Sinusoidal Positional Encoding to Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42dc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = token_emb.shape[0]\n",
    "d_model = token_emb.shape[1]\n",
    "\n",
    "pos_enc = sinusoidal_positional_encoding(seq_len, d_model)\n",
    "\n",
    "# Add position encoding to token embeddings\n",
    "final_emb = token_emb + pos_enc\n",
    "\n",
    "print(\"Final embedding shape:\", final_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042ff95",
   "metadata": {},
   "source": [
    "### 2. Learned Positional Embeddings (GPT-style)\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LearnedPositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long)\n",
    "        return self.pos_emb(positions)  # (seq_len, d_model)\n",
    "\n",
    "max_len = 1024\n",
    "d_model = 768\n",
    "\n",
    "learned_pe = LearnedPositionEmbedding(max_len, d_model)\n",
    "pos_learned = learned_pe(seq_len)\n",
    "\n",
    "print(\"Learned positional embedding shape:\", pos_learned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd6f2e",
   "metadata": {},
   "source": [
    "#### Combine Learned Positional Embeddings with Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_emb_learned = token_emb + pos_learned\n",
    "print(\"Final embedding (learned PE) shape:\", final_emb_learned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbea27",
   "metadata": {},
   "source": [
    "## Attention\n",
    "### Understanding & Visualizing Attention (GPT-2)\n",
    "\n",
    "#### üìò Model Context\n",
    "- **Model:** `GPT-2` (decoder-only Transformer)\n",
    "- **Architecture:** 12 layers √ó 12 attention heads  \n",
    "- **Attention type:** *Causal self-attention* ‚Üí each token can only attend to **previous** tokens (no future look-ahead)\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç What the Heatmap Shows\n",
    "- **Rows (Y-axis):** Query tokens ‚Äî the ones *attending*  \n",
    "- **Columns (X-axis):** Key tokens ‚Äî the ones *being attended to*  \n",
    "- **Bright cells:** High attention weight ‚Üí strong focus/relationship  \n",
    "- **Diagonal line:** Each token attends to itself (self-attention)  \n",
    "- **Dark upper triangle:** Causal mask (future tokens are hidden)\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Data Shapes\n",
    "- `attentions[layer]` ‚Üí shape `[batch, heads, seq_len, seq_len]`  \n",
    "  - Each entry = one attention matrix for a single head  \n",
    "- `last_hidden_state` ‚Üí contextualized token embeddings  \n",
    "- `attentions` ‚Üí relationships between tokens (who ‚Äúlooks‚Äù at whom)\n",
    "\n",
    "---\n",
    "\n",
    "#### üé® Visualization Settings\n",
    "- Color map: `\"magma\"` for strong contrast  \n",
    "- Log scale: `np.log1p(attention)` to highlight smaller differences  \n",
    "- Color range: `vmin=0.0`, `vmax‚âà0.15‚Äì0.25` to make bright regions pop  \n",
    "- Convert tensor before plotting:  \n",
    "  ```python\n",
    "  attention = attention.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde555f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding & Visualizing Attention (GPT-2)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "sentence = \"The dog chased the ball because it was excited.\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "attentions = outputs.attentions  # List of length L (num layers), each element is a tensor of shape (num_heads, seq_len, seq_len)\n",
    "# So attentions[0] ‚Üí first layer, all heads in that layer.\n",
    "# attentions[1] ‚Üí second layer, all heads in that layer, and so on.\n",
    "# attentions[0].shape gives (batch_size, num_heads, seq_len, seq_len). [1] selects the second dimension, which corresponds to number of heads (num_heads).\n",
    "print(f\"Layers: {len(attentions)} | Heads per layer: {attentions[0].shape[1]}\")\n",
    "\n",
    "# Visualize one attention head from the last layer\n",
    "layer = -1  # last layer\n",
    "head = 0   # first attention head\n",
    "\n",
    "attention = attentions[layer][0, head].cpu()\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(np.log1p(attention), xticklabels=tokens, yticklabels=tokens, cmap=\"magma\", square=True, vmin=0.0, vmax=0.15)\n",
    "plt.title(f\"GPT-2 Attention Map (Layer {layer}, Head {head})\")\n",
    "plt.xlabel(\"Key Tokens\")\n",
    "plt.ylabel(\"Query Tokens\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
