{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212d6aad",
   "metadata": {},
   "source": [
    "# Building Blocks of Language Models\n",
    "In this notebook, we will explore:\n",
    "- <b>Tokenization</b>: Breaking text into tokens\n",
    "- <b>Embeddings</b>: Converting tokens to numerical vectors\n",
    "- <b>Positional Encoding</b>: Adding position information\n",
    "- <b>Self-Attention</b>: Using all of the above to understand relationships\n",
    "- <b>Prediction</b>: Generating the next token using the model's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8feb0",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- The model canâ€™t read words directly â€” it only understands numbers. So we need to convert text â†’ tokens â†’ numbers.\n",
    "- tokenize() splits into sub-word pieces.\n",
    "- encode() converts them into numeric IDs.\n",
    "- decode() reverses the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f137009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a small open model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Small Language Models are powerful and efficient!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)\n",
    "print(\"Decoded back:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b60004",
   "metadata": {},
   "source": [
    "### A Simple Tokenizer Implementation (Toy Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    # Lowercase + split on spaces & punctuation\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "    return tokens\n",
    "\n",
    "sample_text = \"Small Language Models are powerful and efficient!\"\n",
    "tokens = basic_tokenizer(sample_text)\n",
    "print(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663791a",
   "metadata": {},
   "source": [
    "### Real GPT-2 BPE tokenizer\n",
    "1. GPT-2 uses subword tokenization (Byte Pair Encoding, BPE) to handle rare words efficiently.\n",
    "2. Tokens are not always whole words; BPE splits or merges based on frequency.\n",
    "3. The tokenizerâ€™s vocabulary is fixed, built before model training. \n",
    "   Tokenizer training is often done on a subset of the full corpus (e.g., 1â€“10% of data), if the corpus is extremely large, to save time.\n",
    "   The subset must be representative of all text the model will encounter.\n",
    "   The vocabulary generated is then fixed and used throughout model training.\n",
    "\n",
    "### Analogy\n",
    "- Think of the vocabulary as a dictionary for the model.\n",
    "- You want the dictionary to cover all common words in the language youâ€™re going to teach the model.\n",
    "- Making a dictionary from Shakespeare when youâ€™re training on Wikipedia would work poorly â€” it wonâ€™t reflect the frequency of words in Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428238c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer (uses real BPE)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Small Language Models are powerful and efficient!\"\n",
    "tokens_hf = hf_tokenizer.tokenize(text)\n",
    "ids_hf = hf_tokenizer.encode(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"\\nHugging Face BPE Tokens:\", tokens_hf)\n",
    "print(\"Token IDs:\", ids_hf)\n",
    "print(\"Decoded back:\", hf_tokenizer.decode(ids_hf))\n",
    "# Decoding some random token ID - Prints a word referred in the tokenizer vocabulary\n",
    "print(\"Random Token:\", hf_tokenizer.decode(18710))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cdcf7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d9d0f",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "### Definition of embedding:\n",
    "* Numerical vector representing an object (word, token, or sentence) in high-dimensional space.\n",
    "* Similar objects â†’ vectors close together; dissimilar â†’ far apart.\n",
    "\n",
    "### Purpose in Transformers / LLMs:\n",
    "* Converts symbolic text into numbers so the model can process it.\n",
    "* Captures semantic meaning and, in last_hidden_state, contextual meaning.\n",
    "\n",
    "#### Example:\n",
    "- Text: \"Small Language Models\"\n",
    "- Tokens: [\"Small\", \"Ä Language\", \"Ä Models\"]\n",
    "- Each token â†’ 768-dimensional vector (GPT-2 small).\n",
    "\n",
    "### Why embeddings are important:\n",
    "* Provide contextualization: combined with attention, the model â€œunderstandsâ€ meaning in context.\n",
    "* Allow semantic similarity: similar words (e.g., â€œkingâ€ & â€œqueenâ€) have similar vectors.\n",
    "* Serve as the starting point for downstream tasks: next-token prediction, classification, etc.\n",
    "\n",
    "### Analogy:\n",
    "* Think of a map of cities\n",
    "* Each city = token\n",
    "* Coordinates = embedding vector\n",
    "* Nearby cities = similar meaning\n",
    "* Moving through the map = attention + feed-forward layers updating embeddings\n",
    "\n",
    "### last_hidden_state:\n",
    "* Tensor of shape [batch_size, seq_len, hidden_size]\n",
    "* Each token has a contextualized embedding vector after passing through Transformer layers.\n",
    "* Before passing through layers â†’ embeddings are basic numeric representations of tokens.\n",
    "* After layers â†’ embeddings are contextualized, rich representations used for predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28677051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Embeddings from GPT-2\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Small Language Models are powerful and efficient!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # shape: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n",
    "print(\"Number of tokens:\", embeddings.shape[1])\n",
    "print(\"Hidden size:\", embeddings.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Embeddings with PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "emb_np = embeddings[0].numpy()  # convert from tensor to numpy\n",
    "\n",
    "# Reduce 768-d to 2D\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(emb_np)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(emb_2d[:,0], emb_2d[:,1])\n",
    "for i, token in enumerate(tokens):\n",
    "    plt.annotate(token, (emb_2d[i,0], emb_2d[i,1]))\n",
    "plt.title(\"2D PCA of GPT-2 Token Embeddings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628e273",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2fb20",
   "metadata": {},
   "source": [
    "## Positional Encoding (Sinusoidal + Learned)\n",
    "Transformers donâ€™t process words sequentially like RNNs.\n",
    "So they need to know where each token is in the sentence.\n",
    "\n",
    "They add a positional vector (e.g., sine/cosine pattern or learned embedding)\n",
    "so â€œThe catâ€ â‰  â€œcat Theâ€.\n",
    "\n",
    "Result = (embedding + position) for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af708ca",
   "metadata": {},
   "source": [
    "### 1. Sinusoidal Positional Encoding\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f95530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encodings of shape (seq_len, d_model)\n",
    "    As described in 'Attention is All You Need'\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    # frequencies: 10000^(2i/d_model)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "    # apply sin to even indices, cos to odd\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db6df2",
   "metadata": {},
   "source": [
    "#### Visualize Sinusoidal Encoding Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq_len = 1024\n",
    "d_model = 768\n",
    "\n",
    "pe = sinusoidal_positional_encoding(seq_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe, aspect='auto', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Sinusoidal Positional Encoding (1024 positions Ã— 768 dims)\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0748e2",
   "metadata": {},
   "source": [
    "#### Extract GPT-2 Token Embeddings for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c276c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"The dog chased the ball.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    token_emb = outputs.last_hidden_state[0]  # shape: (seq_len, d_model)\n",
    "\n",
    "print(\"Token embedding shape:\", token_emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b209f",
   "metadata": {},
   "source": [
    "#### Add Sinusoidal Positional Encoding to Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42dc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = token_emb.shape[0]\n",
    "d_model = token_emb.shape[1]\n",
    "\n",
    "pos_enc = sinusoidal_positional_encoding(seq_len, d_model)\n",
    "\n",
    "# Add position encoding to token embeddings\n",
    "final_emb = token_emb + pos_enc\n",
    "\n",
    "print(\"Final embedding shape:\", final_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042ff95",
   "metadata": {},
   "source": [
    "### 2. Learned Positional Embeddings (GPT-style)\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LearnedPositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long)\n",
    "        return self.pos_emb(positions)  # (seq_len, d_model)\n",
    "\n",
    "max_len = 1024\n",
    "d_model = 768\n",
    "\n",
    "learned_pe = LearnedPositionEmbedding(max_len, d_model)\n",
    "pos_learned = learned_pe(seq_len)\n",
    "\n",
    "print(\"Learned positional embedding shape:\", pos_learned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd6f2e",
   "metadata": {},
   "source": [
    "#### Combine Learned Positional Embeddings with Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_emb_learned = token_emb + pos_learned\n",
    "print(\"Final embedding (learned PE) shape:\", final_emb_learned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072f8d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbea27",
   "metadata": {},
   "source": [
    "## Attention\n",
    "### Attention Visualization (using a real GPT-2 model)\n",
    "\n",
    "#### ðŸ“˜ Model Context\n",
    "- **Model:** `GPT-2` (decoder-only Transformer)\n",
    "- **Architecture:** 12 layers Ã— 12 attention heads  \n",
    "- **Attention type:** *Causal self-attention* â†’ each token can only attend to **previous** tokens (no future look-ahead)\n",
    "\n",
    "\n",
    "#### ðŸ” What the Heatmap Shows\n",
    "- **Rows (Y-axis):** Query tokens â€” the ones *attending*  \n",
    "- **Columns (X-axis):** Key tokens â€” the ones *being attended to*  \n",
    "- **Bright cells:** High attention weight â†’ strong focus/relationship  \n",
    "- **Diagonal line:** Each token attends to itself (self-attention)  \n",
    "- **Dark upper triangle:** Causal mask (future tokens are hidden)\n",
    "\n",
    "\n",
    "#### ðŸ§© Data Shapes\n",
    "- `attentions[layer]` â†’ shape `[batch, heads, seq_len, seq_len]`  \n",
    "  - Each entry = one attention matrix for a single head  \n",
    "- `last_hidden_state` â†’ contextualized token embeddings  \n",
    "- `attentions` â†’ relationships between tokens (who â€œlooksâ€ at whom)\n",
    "\n",
    "\n",
    "#### ðŸŽ¨ Visualization Settings\n",
    "- Color map: `\"magma\"` for strong contrast  \n",
    "- Log scale: `np.log1p(attention)` to highlight smaller differences  \n",
    "- Color range: `vmin=0.0`, `vmaxâ‰ˆ0.15â€“0.25` to make bright regions pop  \n",
    "- Convert tensor before plotting:  \n",
    "  ```python\n",
    "  attention = attention.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde555f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding & Visualizing Attention (GPT-2)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "sentence = \"The dog chased the ball because it was excited.\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "attentions = outputs.attentions  # List of length L (num layers), each element is a tensor of shape (num_heads, seq_len, seq_len)\n",
    "# So attentions[0] â†’ first layer, all heads in that layer.\n",
    "# attentions[1] â†’ second layer, all heads in that layer, and so on.\n",
    "# attentions[0].shape gives (batch_size, num_heads, seq_len, seq_len). [1] selects the second dimension, which corresponds to number of heads (num_heads).\n",
    "print(f\"Layers: {len(attentions)} | Heads per layer: {attentions[0].shape[1]}\")\n",
    "\n",
    "# Visualize one attention head from the last layer\n",
    "layer = -1  # last layer\n",
    "head = 0   # first attention head\n",
    "\n",
    "attention = attentions[layer][0, head].cpu()\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(np.log1p(attention), xticklabels=tokens, yticklabels=tokens, cmap=\"magma\", square=True, vmin=0.0, vmax=0.15)\n",
    "plt.title(f\"GPT-2 Attention Map (Layer {layer}, Head {head})\")\n",
    "plt.xlabel(\"Key Tokens\")\n",
    "plt.ylabel(\"Query Tokens\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef34f86",
   "metadata": {},
   "source": [
    "### Self-Attention Implementation (From Scratch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fa8db",
   "metadata": {},
   "source": [
    "#### 1. Set up\n",
    "We will simulate a very small model with:\n",
    " - Sequence Length (T): 3 tokens\n",
    " - Hidden Size (D): 4 dimensions\n",
    " - Head Dimension (dk): 2 dimensions (We'll use a single head for simplicity).\n",
    " - We need three sets of Learned Weights (WQ, WK, WV), which the model learned during training. These weights transform the input embedding X into Q, K, and V.\n",
    " - The input X is the combined token and positional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Shapes and Input ---\n",
    "# D_model (Hidden size) = 4\n",
    "# d_k (Head dimension) = 2\n",
    "# T (Sequence Length) = 3\n",
    "\n",
    "D_MODEL = 4\n",
    "D_HEAD = 2\n",
    "SEQ_LEN = 3\n",
    "\n",
    "# Simulate the input X (Token Embeddings + Positional Embeddings)\n",
    "# Shape: [SEQ_LEN, D_MODEL] -> [3, 4]\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.5, 0.2, 0.1],  # x1: \"Small\"\n",
    "    [0.1, 1.2, 0.3, 0.5],  # x2: \"Language\"\n",
    "    [0.8, 0.4, 1.1, 0.2]   # x3: \"Models\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Input Embeddings (X) Shape:\", X.shape)\n",
    "\n",
    "# --- 2. Simulate Learned Weights ---\n",
    "# Weights must transform D_MODEL -> D_HEAD\n",
    "# Shape: [D_MODEL, D_HEAD] -> [4, 2]\n",
    "W_Q = torch.randn(D_MODEL, D_HEAD)\n",
    "W_K = torch.randn(D_MODEL, D_HEAD)\n",
    "W_V = torch.randn(D_MODEL, D_HEAD)\n",
    "\n",
    "print(\"Weight Matrix (W_Q) Shape:\", W_Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906f24d",
   "metadata": {},
   "source": [
    "#### 2. Computing Q, K, and V\n",
    "Q, K, and V are generated by simple matrix multiplication:\n",
    "- Q = WQ . X\n",
    "- K = WK . X\n",
    "- V = WV . X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60312423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Generate Q, K, V Matrices ---\n",
    "# Shape: [SEQ_LEN, D_MODEL] * [D_MODEL, D_HEAD] = [SEQ_LEN, D_HEAD]\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"\\\\nQ Matrix (Queries):\\\\n\", Q)\n",
    "print(\"K Matrix (Keys):\\\\n\", K)\n",
    "print(\"V Matrix (Values):\\\\n\", V)\n",
    "print(f\"Q, K, V Shapes: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d1735",
   "metadata": {},
   "source": [
    "#### 3. Computing Attention Scores and Weights\n",
    "\n",
    "This is the heart of the mechanism. The scores are computed by multiplying Q and the transpose of K (K^T).\n",
    "1. Scores: Scores=Qâ‹…K^T\n",
    " - Shape: [3,2]â‹…[2,3]=[3,3] (The TÃ—T attention matrix!)\n",
    "\n",
    "2. Weights: Weights=Softmax(Scores/sqroot(d_k))\n",
    " - The devision by sqrt(d_k) (scaling factor) is to normalize the scores and prevent the softmax from becoming too large.\n",
    " - The Softmax converts the scores into probabilities (attention weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Compute Attention Scores (Q * K^T) ---\n",
    "# Scale factor (square root of the head dimension)\n",
    "scale_factor = torch.sqrt(torch.tensor(D_HEAD, dtype=torch.float32))\n",
    "\n",
    "# Scores shape: [SEQ_LEN, SEQ_LEN] -> [3, 3]\n",
    "Scores = Q @ K.T / scale_factor\n",
    "\n",
    "print(\"\\\\nAttention Scores (Q Â· K^T / sqrt(d_k)):\\\\n\", Scores)\n",
    "\n",
    "# --- 5. Apply Causal Mask (for GPT-style attention) ---\n",
    "# We use float('-inf') to mask (hide) future tokens.\n",
    "# Tokens can only attend to tokens that came before them.\n",
    "mask = torch.triu(torch.ones(SEQ_LEN, SEQ_LEN) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Apply mask to scores\n",
    "Masked_Scores = Scores + mask\n",
    "\n",
    "print(\"\\\\nMasked Attention Scores (Causal):\\\\n\", Masked_Scores)\n",
    "\n",
    "# --- 6. Compute Attention Weights (Softmax) ---\n",
    "# Softmax ensures weights sum to 1 for each row (Query)\n",
    "Attention_Weights = F.softmax(Masked_Scores, dim=-1)\n",
    "\n",
    "print(\"\\\\nAttention Weights (Softmax):\\\\n\", Attention_Weights)\n",
    "\n",
    "# Check that rows sum to 1 (for first token, only one non-zero entry)\n",
    "print(\"Row Sums (Should be 1):\", torch.sum(Attention_Weights, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8e62f",
   "metadata": {},
   "source": [
    "#### 4. Computing the Attention Output\n",
    "\n",
    "The final attention output is the weighted sum of the Value vectors. This is where the contextual information is merged.\n",
    " - Output=AttentionÂ Weightsâ‹…V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ec918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Compute Final Attention Output ---\n",
    "# Output shape: [SEQ_LEN, SEQ_LEN] * [SEQ_LEN, D_HEAD] = [SEQ_LEN, D_HEAD]\n",
    "Attention_Output = Attention_Weights @ V\n",
    "\n",
    "print(\"\\\\nAttention Output (Weighted Sum of V):\\\\n\", Attention_Output)\n",
    "print(\"Attention Output Shape:\", Attention_Output.shape)\n",
    "\n",
    "# Example Interpretation:\n",
    "# The first row of Attention_Output is the contextualized vector for \"Small\".\n",
    "# Because of the causal mask, the first token can only attend to itself, \n",
    "# so the first row of Attention_Output should be close to the first row of V.\n",
    "\n",
    "# Check (V[0] vs Output[0]):\n",
    "print(\"\\\\nV[0] (for 'Small'):\", V[0])\n",
    "print(\"Output[0] ('Small' Contextualized):\", Attention_Output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe19f4",
   "metadata": {},
   "source": [
    "#### 5. Add & Layer Normalization 1 (After MHA)\n",
    "\n",
    "After the MHA block computes the contextualized output, we apply two crucial mechanisms that ensure the model can train deeply and effectively: the Residual Connection (Add) and Layer Normalization (Norm).\n",
    "\n",
    "##### 5.1. The Residual Connection (Add)\n",
    "\n",
    "This step takes the output from the Multi-Head Attention layer and simply **adds** it to the layer's original input.\n",
    "\n",
    "$$\\mathbf{Y}_{\\text{res}} = X + \\text{MHA}_{\\text{Output}}$$\n",
    "\n",
    "* **Purpose:** This creates a **shortcut** around the attention block, allowing the gradient (the learning signal) to flow directly through the network during training. Without this shortcut, gradients would vanish in deep models, preventing the first layers from learning. It ensures that the model can, at worst, maintain its previous state of knowledge if the attention layer provides no meaningful update.\n",
    "\n",
    "##### 5.2. Layer Normalization (Norm)\n",
    "\n",
    "Layer Normalization is applied immediately after the residual addition.\n",
    "\n",
    "* **Purpose:** LayerNorm stabilizes the activation values. The MHA block involves intense matrix math (dot products and softmax) that can cause the resulting vector's numerical range to shift wildly.\n",
    "* **Mechanism:** LayerNorm calculates the mean and variance across the entire **feature dimension** ($D_{\\text{model}}$) for **each token individually**. It then centers the data ($\\mu=0$) and scales it ($\\sigma^2=1$).\n",
    "* **Result:** This ensures that the input to the next major block (the Feed-Forward Network) is always consistent and within a stable range, which drastically speeds up training convergence.\n",
    "\n",
    "**In essence, the \"Add & Norm\" sequence is about stability:** the **Add** stabilizes the *learning process*, and the **Norm** stabilizes the *data values*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Continuing from the Attention Output (Attention_Output) ---\n",
    "# X is the original input embedding tensor from the beginning of the layer\n",
    "\n",
    "# 1. Residual Connection (ADD)\n",
    "# Requires X to have the same shape as Attention_Output.\n",
    "# NOTE: In a real model, the output projection makes Attention_Output return to D_MODEL (4 in our case).\n",
    "# We simulate this by adjusting the shape of Attention_Output from [3, 2] to [3, 4] \n",
    "# and then adding it back to X [3, 4].\n",
    "\n",
    "# Let's re-use X (Input Embeddings) from earlier for the residual path:\n",
    "# X shape: [3, 4]\n",
    "# We must first project Attention_Output [3, 2] back to [3, 4] to add it to X.\n",
    "# We simulate the Output Projection W_O here for simplicity.\n",
    "\n",
    "W_O_sim = torch.randn(D_HEAD, D_MODEL) # [2, 4]\n",
    "MHA_Output = Attention_Output @ W_O_sim # [3, 2] @ [2, 4] = [3, 4]\n",
    "\n",
    "# Add & Norm 1\n",
    "Y_res_1 = X + MHA_Output \n",
    "print(\"Residual Output 1 Shape (X + MHA_Output):\", Y_res_1.shape)\n",
    "\n",
    "# 2. Layer Normalization\n",
    "# Normalizes across the feature dimension (dim=1, D_MODEL=4)\n",
    "ln1 = torch.nn.LayerNorm(D_MODEL) \n",
    "Output_LN1 = ln1(Y_res_1)\n",
    "\n",
    "print(\"LayerNorm 1 Output (Input to FFN) Shape:\", Output_LN1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9296b5",
   "metadata": {},
   "source": [
    "#### 6. The Feed-Forward Network (FFN)\n",
    "\n",
    "The Feed-Forward Network is the second major sub-layer in the Transformer block, positioned right after the first Add & Layer Normalization.\n",
    "\n",
    "##### 6.1. Conceptual Role\n",
    "\n",
    "While the Multi-Head Attention (MHA) block is responsible for **mixing information *across* the entire sequence** (contextualizing a token based on others), the FFN works **independently on each token's embedding** to deepen its feature representation.\n",
    "\n",
    "* **Non-Linearity:** The FFN applies critical non-linear transformations (using the $\\text{GELU}$ activation function). This is essential because the world is complex, and the model needs non-linear functions to learn intricate relationships and patterns.\n",
    "* **Feature Expansion:** The FFN expands the token's feature vector to a higher dimension (e.g., $D_{\\text{model}} \\rightarrow 4 \\times D_{\\text{model}}$) and then contracts it back down. This expansion-contraction forces the model to learn a richer, concentrated set of features for each token.\n",
    "\n",
    "##### 6.2. The Two-Layer Structure\n",
    "\n",
    "The FFN is a standard two-layer Multi-Layer Perceptron (MLP):\n",
    "\n",
    "1.  **Expansion Layer ($\\mathbf{W}_{\\text{in}}$):** Takes the normalized input vector (e.g., 768-dim) and expands it fourfold (e.g., to 3072-dim). An activation function ($\\text{GELU}$) is applied here to introduce non-linearity.\n",
    "2.  **Contraction Layer ($\\mathbf{W}_{\\text{out}}$):** Takes the expanded vector (e.g., 3072-dim) and projects it back down to the original hidden size (e.g., 768-dim).\n",
    "\n",
    "$$\\text{FFN}(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{W}_{\\text{in}} \\xrightarrow{\\text{GELU}} \\mathbf{y}' \\cdot \\mathbf{W}_{\\text{out}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define FFN Dimensions ---\n",
    "# D_MODEL was set to 4 in our simulation\n",
    "D_MODEL = 4\n",
    "D_FFN = D_MODEL * 4 # Standard expansion factor of 4\n",
    "print(f\"FFN will expand from {D_MODEL} to {D_FFN} and back to {D_MODEL}\")\n",
    "\n",
    "# --- FFN Implementation Class ---\n",
    "class SimpleFFN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Feed-Forward Network (FFN) with expansion and contraction layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ffn):\n",
    "        super().__init__()\n",
    "        # 1. Expansion layer: D_MODEL -> D_FFN\n",
    "        self.linear_in = torch.nn.Linear(d_model, d_ffn)\n",
    "        # 2. Contraction layer: D_FFN -> D_MODEL\n",
    "        self.linear_out = torch.nn.Linear(d_ffn, d_model)\n",
    "        # Non-linearity (GPT models typically use GELU)\n",
    "        self.activation = torch.nn.GELU() \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply expansion and activation\n",
    "        x = self.linear_in(x)\n",
    "        x = self.activation(x)\n",
    "        # Apply contraction\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "\n",
    "# Assuming 'Output_LN1' is the tensor produced after the first Layer Norm (Add & Norm 1)\n",
    "# Re-define Output_LN1 for execution context, if the previous code cells were not run\n",
    "# Since we cannot run the previous cells, we'll create a dummy tensor with the expected shape [3, 4]\n",
    "\n",
    "# --- IMPORTANT: Ensure Output_LN1 exists from previous run or define it ---\n",
    "# You should ensure your previous cells run up to the first Layer Norm.\n",
    "# For demonstration purposes here, we'll use a placeholder variable:\n",
    "try:\n",
    "    # This assumes Output_LN1 was generated successfully in the previous cell block\n",
    "    print(f\"Input to FFN (Output_LN1) successfully loaded with shape: {Output_LN1.shape}\")\n",
    "except NameError:\n",
    "    # If the variable is not defined (e.g., running this cell isolated), use a placeholder:\n",
    "    print(\"WARNING: Output_LN1 not found. Creating placeholder tensor for FFN input.\")\n",
    "    Output_LN1 = torch.randn(3, 4) \n",
    "\n",
    "# --- 1. Run the FFN ---\n",
    "ffn = SimpleFFN(D_MODEL, D_FFN)\n",
    "FFN_Output = ffn(Output_LN1)\n",
    "\n",
    "print(\"\\\\n--- FFN Execution ---\")\n",
    "print(\"FFN Output Shape:\", FFN_Output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722e468",
   "metadata": {},
   "source": [
    "#### 7. Add & Layer Normalization 2 (The Final Steps)\n",
    "\n",
    "After the FFN, the output is passed through the same stabilization steps again:\n",
    "\n",
    "1.  **Residual Connection (Add):** The FFN output is added to the FFN's input ($\\text{Output}_{\\text{LN1}}$).\n",
    "2.  **Layer Normalization (Norm):** The result is normalized.\n",
    "\n",
    "This final `Add & Norm` ensures that the resulting contextualized vector is stable and ready to be passed to the next Transformer layer in the stack.\n",
    "\n",
    "**Key Takeaway:** The FFN works on *what* a token is, while the MHA works on *where* a token is in context. Together, they create a powerful, context-aware representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Add & Layer Norm 2 (The Final Steps) ---\n",
    "\n",
    "# 2.1. Residual Connection (ADD)\n",
    "# The FFN output is added to its input (Output_LN1)\n",
    "Y_res_2 = Output_LN1 + FFN_Output\n",
    "\n",
    "print(\"Residual Output 2 Shape (Output_LN1 + FFN_Output):\", Y_res_2.shape)\n",
    "\n",
    "\n",
    "# 2.2. Layer Normalization\n",
    "ln2 = torch.nn.LayerNorm(D_MODEL) \n",
    "Final_Layer_Output = ln2(Y_res_2)\n",
    "\n",
    "print(\"Final Layer Output Shape:\", Final_Layer_Output.shape)\n",
    "print(\"--- Layer Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c101b7",
   "metadata": {},
   "source": [
    "## Prediction - Logits and Next Token Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268206d",
   "metadata": {},
   "source": [
    "The ultimate function of a Generative Language Model (LLM) is to predict the **next token** in a sequence. This prediction is based on the final, contextualized embedding vector produced by the very last Transformer layer.\n",
    "\n",
    "### 1. The Final Transformation: Logits\n",
    "\n",
    "The prediction is not a word itself, but a set of raw numerical scores called **Logits**.\n",
    "\n",
    "1.  **Logits Projection (The LM Head):** The final contextualized embedding, which is $D_{\\text{model}}$-dimensional (e.g., 768), is projected onto the size of the model's entire vocabulary (e.g., 50,257 for GPT-2). This is done via a final, linear layer known as the **Language Modeling Head**.\n",
    "2.  **Output:** The Logits tensor has a shape of $[1, \\text{Vocab Size}]$. Each number in this tensor represents the raw, unnormalized score for how likely the corresponding token ID is to be the next token.\n",
    "\n",
    "### 2 Softmax and Sampling\n",
    "\n",
    "The Logits must be converted into a usable prediction:\n",
    "\n",
    "1.  **Softmax:** The $\\text{Softmax}$ function is applied to the Logits to transform them into a probability distribution. The resulting probabilities sum up to 1, showing the confidence score for every possible next token.\n",
    "2.  **Sampling:** The model then uses these probabilities to choose the next token ID.\n",
    "    * **Greedy Decoding:** Choosing the token with the absolute highest probability (the `torch.argmax` method).\n",
    "    * **Sampling:** Choosing a token based on the probabilities (e.g., if \"the\" has 80% and \"a\" has 10%, the model might randomly pick \"the\" 8 out of 10 times).\n",
    "\n",
    "The final loop of the Transformer is:\n",
    "$$\\text{Contextualized Vector} \\xrightarrow{\\text{LM Head}} \\text{Logits} \\xrightarrow{\\text{Softmax}} \\text{Probabilities} \\xrightarrow{\\text{Sampling}} \\text{New Token}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The dog chased the ball because it\n",
      "\\nLogits Shape (Batch, Seq_Len, Vocab_Size): torch.Size([1, 7, 50257])\n",
      "Logits for next token prediction Shape: torch.Size([50257])\n",
      "--------------------------------------------------\n",
      "Top Logit Value: -115.58\n",
      "Greedy Prediction (Highest Probability): ' was'\n",
      "Full Sequence: 'The dog chased the ball because it was'\n",
      "--------------------------------------------------\n",
      "\\nTop 5 Next Token Predictions:\n",
      "  1. Token: ' was' | Probability: 42.30%\n",
      "  2. Token: ' had' | Probability: 8.79%\n",
      "  3. Token: ' didn' | Probability: 3.87%\n",
      "  4. Token: ' wanted' | Probability: 2.91%\n",
      "  5. Token: ' wasn' | Probability: 2.67%\n",
      "  6. Token: ' could' | Probability: 1.80%\n",
      "  7. Token: ' seemed' | Probability: 1.74%\n",
      "  8. Token: ' ran' | Probability: 1.60%\n",
      "  9. Token: ' would' | Probability: 1.48%\n",
      "  10. Token: ' looked' | Probability: 1.44%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# AutoModelForCausalLM includes the final prediction layer (LM Head)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# Input sentence for the model to continue\n",
    "sentence = \"The dog chased the ball because it\"\n",
    "print(\"Input:\", sentence)\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# --- 1. Forward Pass to get Logits ---\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # The output object for AutoModelForCausalLM contains the logits\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(\"\\\\nLogits Shape (Batch, Seq_Len, Vocab_Size):\", logits.shape)\n",
    "\n",
    "# --- 2. Focus on the LAST Token's Logits ---\n",
    "# Logits for the last token are used to predict the next word.\n",
    "last_token_logits = logits[0, -1, :] \n",
    "\n",
    "print(\"Logits for next token prediction Shape:\", last_token_logits.shape)\n",
    "\n",
    "# --- 3. Convert Logits to Probabilities (Softmax) and Sample ---\n",
    "\n",
    "# Apply softmax to convert raw scores into a probability distribution\n",
    "probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# Find the token with the highest probability (Greedy Decoding)\n",
    "next_token_id_greedy = torch.argmax(probabilities).item()\n",
    "\n",
    "# --- 4. Decode and Display Prediction ---\n",
    "next_token_greedy = tokenizer.decode(next_token_id_greedy)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Top Logit Value: {last_token_logits.max().item():.2f}\")\n",
    "print(f\"Greedy Prediction (Highest Probability): '{next_token_greedy}'\")\n",
    "print(f\"Full Sequence: '{sentence}{next_token_greedy}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Bonus: Show Top 5 Predictions ---\n",
    "top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "print(\"\\\\nTop 5 Next Token Predictions:\")\n",
    "for i in range(5):\n",
    "    token = tokenizer.decode(top_5_indices[i].item())\n",
    "    prob = top_5_probs[i].item()\n",
    "    print(f\"  {i+1}. Token: '{token}' | Probability: {prob*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
